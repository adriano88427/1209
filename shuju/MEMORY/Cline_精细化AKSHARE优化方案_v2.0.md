# Cline_ç²¾ç»†åŒ–AKSHAREä¼˜åŒ–æ–¹æ¡ˆ_v2.0

## æ·±åº¦é—®é¢˜åˆ†æ

### å½“å‰ä»£ç çš„APIè°ƒç”¨æ¨¡å¼åˆ†æ
**æ¯åªè‚¡ç¥¨éœ€è¦10-11æ¬¡APIè°ƒç”¨ï¼š**
1. `ak.stock_zh_a_hist()` - å†å²è‚¡ä»·æ•°æ®ï¼ˆ1æ¬¡ï¼‰
2. `ak.stock_individual_info_em()` - æµé€šè‚¡æœ¬ï¼ˆ1æ¬¡ï¼‰+ ä¸Šå¸‚æ—¶é—´ï¼ˆ1æ¬¡ï¼‰
3. `ak.stock_management_change_ths()` - é«˜ç®¡æŒè‚¡å˜åŠ¨ï¼ˆ1æ¬¡ï¼‰
4. `ak.stock_yzxdr_em()` - ä¸€è‡´è¡ŒåŠ¨äººæ•°æ®ï¼ˆ1æ¬¡ï¼‰**ã€å¯æ‰¹é‡ã€‘**
5. `ak.stock_gdfx_top_10_em()` - å†å²åå¤§è‚¡ä¸œï¼ˆ1æ¬¡ï¼‰
6. `ak.stock_gdfx_free_top_10_em()` - åå¤§æµé€šè‚¡ä¸œï¼ˆ1æ¬¡ï¼‰
7. `ak.stock_report_fund_hold()` - æœºæ„æŒä»“ç­‰ï¼ˆ6æ¬¡ï¼‰**ã€å¯æ‰¹é‡ã€‘**

### å…³é”®æ€§èƒ½ç“¶é¢ˆ
1. **ä¸€è‡´è¡ŒåŠ¨äººé‡å¤æŸ¥è¯¢**ï¼š`ak.stock_yzxdr_em(date=date)` æ¯åªè‚¡ç¥¨éƒ½åœ¨è°ƒç”¨ï¼Œä½†è¯¥APIè¿”å›å…¨å¸‚åœºæ•°æ®
2. **æœºæ„æŒä»“é‡å¤æŸ¥è¯¢**ï¼šæ¯åªè‚¡ç¥¨çš„æœºæ„æŒä»“æ•°æ®éƒ½å•ç‹¬æŸ¥è¯¢å…¨å¸‚åœºæ•°æ®
3. **æ— è¯·æ±‚å¤´ç®¡ç†**ï¼šç¼ºä¹User-Agentè½®æ¢ã€Refererä¼ªè£…ç­‰åçˆ¬ç­–ç•¥
4. **é”™è¯¯æ¢å¤æœºåˆ¶è–„å¼±**ï¼šè¢«IPå°ç¦åæ— æ³•è‡ªåŠ¨æ¢å¤
5. **å¹¶å‘æ§åˆ¶ç¼ºå¤±**ï¼šæ‰€æœ‰è¯·æ±‚ä¸²è¡Œæ‰§è¡Œï¼Œæ•ˆç‡ä½ä¸‹

## ç²¾ç»†åŒ–ä¼˜åŒ–ç­–ç•¥

### 1. é›¶é‡å¤æ•°æ®è·å–æ¶æ„

#### 1.1 ä¸€è‡´è¡ŒåŠ¨äººæ•°æ®ä¼˜åŒ–
```python
class ConsistentDataManager:
    def __init__(self, target_date):
        self.target_date = target_date
        self.yzxdr_cache = {}
        self.fund_hold_cache = {}
        self.request_count = 0
        
    def get_yzxdr_data(self):
        """é›¶é‡å¤è·å–ä¸€è‡´è¡ŒåŠ¨äººæ•°æ®"""
        if not self.yzxdr_cache:
            print(f"ğŸ“¡ æ‰¹é‡è·å–ä¸€è‡´è¡ŒåŠ¨äººæ•°æ® (æ—¥æœŸ: {self.target_date})...")
            start_time = time.time()
            
            self.yzxdr_cache = ak.stock_yzxdr_em(date=self.target_date)
            print(f"âœ… ä¸€è‡´è¡ŒåŠ¨äººæ•°æ®è·å–å®Œæˆï¼Œè€—æ—¶: {time.time()-start_time:.2f}ç§’")
            print(f"ğŸ“Š è·å–åˆ° {len(self.yzxdr_cache)} æ¡è®°å½•")
            
        return self.yzxdr_cache
    
    def get_fund_hold_data(self, hold_type):
        """é›¶é‡å¤è·å–æœºæ„æŒä»“æ•°æ®"""
        if hold_type not in self.fund_hold_cache:
            print(f"ğŸ“¡ æ‰¹é‡è·å– {hold_type} æ•°æ® (æ—¥æœŸ: {self.target_date})...")
            start_time = time.time()
            
            self.fund_hold_cache[hold_type] = ak.stock_report_fund_hold(
                symbol=hold_type, 
                date=self.target_date
            )
            print(f"âœ… {hold_type} æ•°æ®è·å–å®Œæˆï¼Œè€—æ—¶: {time.time()-start_time:.2f}ç§’")
            print(f"ğŸ“Š è·å–åˆ° {len(self.fund_hold_cache[hold_type])} æ¡è®°å½•")
            
        return self.fund_hold_cache[hold_type]
    
    def filter_stock_data(self, stock_code):
        """ä»æ‰¹é‡æ•°æ®ä¸­ç­›é€‰ç›®æ ‡è‚¡ç¥¨"""
        yzxdr_data = self.get_yzxdr_data()
        stock_yzxdr = yzxdr_data[yzxdr_data["è‚¡ç¥¨ä»£ç "] == stock_code]
        return stock_yzxdr
```

#### 1.2 è‚¡ç¥¨ä¿¡æ¯æ‰¹é‡è·å–ä¼˜åŒ–
```python
def batch_get_stock_info_optimized(stock_codes, batch_size=20):
    """ä¼˜åŒ–çš„æ‰¹é‡è‚¡ç¥¨ä¿¡æ¯è·å–"""
    cache_file = f"stock_info_cache_{stock_codes[0]}_{stock_codes[-1]}.pkl"
    
    # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
    if os.path.exists(cache_file):
        with open(cache_file, 'rb') as f:
            return pickle.load(f)
    
    stock_info_dict = {}
    
    for i in range(0, len(stock_codes), batch_size):
        batch_codes = stock_codes[i:i+batch_size]
        print(f"ğŸ“¦ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹è‚¡ç¥¨ä¿¡æ¯ ({len(batch_codes)}åª)")
        
        for code in batch_codes:
            try:
                info = ak.stock_individual_info_em(symbol=code)
                if not info.empty:
                    stock_info_dict[code] = dict(zip(info["item"], info["value"]))
                time.sleep(0.5)  # æ‰¹å†…å»¶è¿Ÿ
            except Exception as e:
                print(f"âš ï¸ è·å–{code}ä¿¡æ¯å¤±è´¥: {e}")
                stock_info_dict[code] = {}
        
        # æ‰¹é—´å»¶è¿Ÿ
        time.sleep(2)
    
    # ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜
    with open(cache_file, 'wb') as f:
        pickle.dump(stock_info_dict, f)
    
    return stock_info_dict
```

### 2. æ™ºèƒ½åçˆ¬ç­–ç•¥

#### 2.1 è¯·æ±‚å¤´ä¼ªè£…ç³»ç»Ÿ
```python
import random
from fake_useragent import UserAgent

class RequestHeaderManager:
    def __init__(self):
        self.ua = UserAgent()
        self.header_pools = [
            {
                'User-Agent': self.ua.chrome,
                'Referer': 'https://www.eastmoney.com/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            },
            {
                'User-Agent': self.ua.firefox,
                'Referer': 'https://stock.eastmoney.com/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            },
            {
                'User-Agent': self.ua.safari,
                'Referer': 'https://www.eastmoney.com/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        ]
    
    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return random.choice(self.header_pools)
    
    def rotate_headers(self, request_func, *args, **kwargs):
        """å¸¦è¯·æ±‚å¤´è½®è°ƒçš„å‡½æ•°æ‰§è¡Œ"""
        for _ in range(3):  # æœ€å¤šå°è¯•3ç§ä¸åŒçš„è¯·æ±‚å¤´
            headers = self.get_random_headers()
            try:
                # å®é™…å®ç°éœ€è¦åœ¨akshareä¸­æ³¨å…¥è¯·æ±‚å¤´
                result = request_func(*args, **kwargs)
                return result
            except Exception as e:
                if "403" in str(e) or "429" in str(e):
                    print(f"âš ï¸ è¯·æ±‚å¤´è¢«è¯†åˆ«ï¼Œåˆ‡æ¢è¯·æ±‚å¤´é‡è¯•...")
                    time.sleep(random.uniform(1, 3))
                    continue
                else:
                    raise e
        
        raise Exception("æ‰€æœ‰è¯·æ±‚å¤´éƒ½å¤±è´¥äº†")
```

#### 2.2 IPè½®æ¢å’Œå»¶è¿Ÿæ§åˆ¶
```python
import requests

class AntiCrawlManager:
    def __init__(self, proxy_list=None):
        self.proxy_list = proxy_list or []
        self.current_proxy_index = 0
        self.request_times = []
        self.blocked_requests = 0
        
    def get_next_proxy(self):
        """è·å–ä¸‹ä¸€ä¸ªä»£ç†IP"""
        if not self.proxy_list:
            return None
        
        proxy = self.proxy_list[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_list)
        return proxy
    
    def adaptive_delay(self):
        """è‡ªé€‚åº”å»¶è¿Ÿç­–ç•¥"""
        now = time.time()
        # æ¸…é™¤5åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
        self.request_times = [t for t in self.request_times if now - t < 300]
        
        # åŸºç¡€å»¶è¿Ÿ
        base_delay = 1.0
        
        # æ ¹æ®è¯·æ±‚é¢‘ç‡è°ƒæ•´å»¶è¿Ÿ
        if len(self.request_times) > 100:
            delay = base_delay * 2  # è¯·æ±‚è¿‡å¤šæ—¶åŠ å€å»¶è¿Ÿ
        elif len(self.request_times) > 50:
            delay = base_delay * 1.5
        else:
            delay = base_delay
        
        # æ·»åŠ éšæœºæŠ–åŠ¨
        delay += random.uniform(0, 0.5)
        
        time.sleep(delay)
        self.request_times.append(now)
    
    def detect_block(self, response):
        """æ£€æµ‹IPæ˜¯å¦è¢«å°"""
        if response.status_code in [403, 429]:
            self.blocked_requests += 1
            return True
        return False
```

### 3. å¤šå±‚æ¬¡ç¼“å­˜ç³»ç»Ÿ

#### 3.1 åˆ†å±‚ç¼“å­˜ç­–ç•¥
```python
class LayeredCache:
    def __init__(self):
        # L1: å†…å­˜ç¼“å­˜ - å½“å‰è¿è¡Œå†…å­˜
        self.l1_cache = {}
        
        # L2: æ–‡ä»¶ç¼“å­˜ - å½“å‰æ—¥æœŸçš„ä¸´æ—¶ç¼“å­˜
        self.l2_cache_dir = f"cache_l2_{datetime.now().strftime('%Y%m%d')}"
        os.makedirs(self.l2_cache_dir, exist_ok=True)
        
        # L3: é•¿æœŸç¼“å­˜ - å†å²æ•°æ®ç¼“å­˜
        self.l3_cache_dir = "cache_l3_longterm"
        os.makedirs(self.l3_cache_dir, exist_ok=True)
        
        # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        self.cache_expiry = {
            'stock_info': 24 * 3600,      # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ï¼š24å°æ—¶
            'yzxdr_data': 12 * 3600,      # ä¸€è‡´è¡ŒåŠ¨äººæ•°æ®ï¼š12å°æ—¶  
            'fund_hold': 6 * 3600,        # æœºæ„æŒä»“æ•°æ®ï¼š6å°æ—¶
            'price_data': 1 * 3600,       # ä»·æ ¼æ•°æ®ï¼š1å°æ—¶
            'holder_data': 24 * 3600,     # è‚¡ä¸œæ•°æ®ï¼š24å°æ—¶
        }
    
    def get_cached_data(self, cache_key, cache_type, max_age=None):
        """è·å–ç¼“å­˜æ•°æ®"""
        # L1ç¼“å­˜æ£€æŸ¥
        if cache_key in self.l1_cache:
            cache_time, data = self.l1_cache[cache_key]
            if not self._is_expired(cache_time, max_age or self.cache_expiry.get(cache_type, 3600)):
                return data
        
        # L2ç¼“å­˜æ£€æŸ¥
        l2_file = os.path.join(self.l2_cache_dir, f"{cache_key}.pkl")
        if os.path.exists(l2_file):
            with open(l2_file, 'rb') as f:
                cache_time, data = pickle.load(f)
            if not self._is_expired(cache_time, max_age or self.cache_expiry.get(cache_type, 3600)):
                self.l1_cache[cache_key] = (cache_time, data)  # å‡çº§åˆ°L1
                return data
        
        # L3ç¼“å­˜æ£€æŸ¥ï¼ˆä»…ç”¨äºé•¿æœŸæ•°æ®ï¼‰
        if cache_type in ['stock_info', 'holder_data']:
            l3_file = os.path.join(self.l3_cache_dir, f"{cache_key}.pkl")
            if os.path.exists(l3_file):
                with open(l3_file, 'rb') as f:
                    cache_time, data = pickle.load(f)
                self.l1_cache[cache_key] = (cache_time, data)
                return data
        
        return None
    
    def set_cached_data(self, cache_key, cache_type, data):
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        cache_time = time.time()
        self.l1_cache[cache_key] = (cache_time, data)
        
        # ä¿å­˜åˆ°L2ç¼“å­˜
        l2_file = os.path.join(self.l2_cache_dir, f"{cache_key}.pkl")
        with open(l2_file, 'wb') as f:
            pickle.dump((cache_time, data), f)
        
        # é•¿æœŸæ•°æ®ä¿å­˜åˆ°L3ç¼“å­˜
        if cache_type in ['stock_info', 'holder_data']:
            l3_file = os.path.join(self.l3_cache_dir, f"{cache_key}.pkl")
            with open(l3_file, 'wb') as f:
                pickle.dump((cache_time, data), f)
    
    def _is_expired(self, cache_time, max_age):
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        return time.time() - cache_time > max_age
```

### 4. å¹¶å‘å¤„ç†ä¼˜åŒ–

#### 4.1 å¼‚æ­¥æ•°æ®è·å–
```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

class AsyncDataFetcher:
    def __init__(self, max_workers=5):
        self.max_workers = max_workers
        self.cache = LayeredCache()
        self.anti_crawl = AntiCrawlManager()
        
    async def fetch_stock_data_async(self, stock_code):
        """å¼‚æ­¥è·å–è‚¡ç¥¨æ•°æ®"""
        loop = asyncio.get_event_loop()
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡çš„akshareè°ƒç”¨
        with ThreadPoolExecutor(max_workers=1) as executor:
            tasks = [
                loop.run_in_executor(executor, self._fetch_single_data, stock_code, data_type)
                for data_type in ['price', 'info', 'holders', 'funds']
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return self._process_results(stock_code, results)
    
    def _fetch_single_data(self, stock_code, data_type):
        """å•æ¬¡æ•°æ®è·å–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        self.anti_crawl.adaptive_delay()
        
        try:
            if data_type == 'price':
                return self._get_price_data(stock_code)
            elif data_type == 'info':
                return self._get_stock_info(stock_code)
            elif data_type == 'holders':
                return self._get_holder_data(stock_code)
            elif data_type == 'funds':
                return self._get_fund_data(stock_code)
        except Exception as e:
            print(f"âŒ è·å–{data_type}æ•°æ®å¤±è´¥ ({stock_code}): {e}")
            return None
    
    async def batch_process_stocks(self, stock_codes):
        """æ‰¹é‡å¼‚æ­¥å¤„ç†è‚¡ç¥¨"""
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def process_with_semaphore(stock_code):
            async with semaphore:
                return await self.fetch_stock_data_async(stock_code)
        
        tasks = [process_with_semaphore(code) for code in stock_codes]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [r for r in results if not isinstance(r, Exception)]
```

### 5. é”™è¯¯æ¢å¤å’Œç›‘æ§

#### 5.1 æ™ºèƒ½é”™è¯¯æ¢å¤
```python
class ErrorRecoveryManager:
    def __init__(self):
        self.error_stats = {}
        self.recovery_strategies = {
            '403': self._handle_ip_banned,
            '429': self._handle_rate_limit,
            'timeout': self._handle_timeout,
            'connection': self._handle_connection_error,
        }
    
    def handle_error(self, error, context):
        """æ™ºèƒ½é”™è¯¯å¤„ç†"""
        error_type = self._classify_error(error)
        self.error_stats[error_type] = self.error_stats.get(error_type, 0) + 1
        
        strategy = self.recovery_strategies.get(error_type)
        if strategy:
            return strategy(error, context)
        else:
            raise error
    
    def _handle_ip_banned(self, error, context):
        """IPè¢«å°å¤„ç†"""
        print("ğŸš¨ æ£€æµ‹åˆ°IPè¢«å°ï¼Œå¯åŠ¨æ¢å¤ç¨‹åº...")
        time.sleep(30)  # é•¿æ—¶é—´ç­‰å¾…
        
        # åˆ‡æ¢ä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(self, 'anti_crawl') and self.anti_crawl.proxy_list:
            new_proxy = self.anti_crawl.get_next_proxy()
            print(f"ğŸ”„ åˆ‡æ¢åˆ°æ–°ä»£ç†: {new_proxy}")
        
        # æ¸…é™¤è¯·æ±‚å¤´ç¼“å­˜
        if hasattr(self, 'header_manager'):
            self.header_manager.header_pools = []
        
        return 'retry'
    
    def _handle_rate_limit(self, error, context):
        """é¢‘ç‡é™åˆ¶å¤„ç†"""
        print("ğŸš¨ æ£€æµ‹åˆ°é¢‘ç‡é™åˆ¶ï¼Œåº”ç”¨æŒ‡æ•°é€€é¿...")
        delay = min(2 ** self.error_stats.get('429', 1), 60)
        time.sleep(delay)
        return 'retry'
    
    def _handle_timeout(self, error, context):
        """è¶…æ—¶å¤„ç†"""
        print("ğŸš¨ ç½‘ç»œè¶…æ—¶ï¼Œå‡å°‘å¹¶å‘æ•°...")
        if hasattr(context, 'reduce_concurrency'):
            context.reduce_concurrency()
        time.sleep(5)
        return 'retry'
```

#### 5.2 å®æ—¶ç›‘æ§å’Œå‘Šè­¦
```python
class PerformanceMonitor:
    def __init__(self):
        self.start_time = time.time()
        self.request_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'blocked_requests': 0,
            'avg_response_time': 0,
        }
        self.performance_history = []
        
    def log_request(self, success, response_time, is_blocked=False):
        """è®°å½•è¯·æ±‚æ€§èƒ½"""
        self.request_stats['total_requests'] += 1
        
        if success:
            self.request_stats['successful_requests'] += 1
        elif is_blocked:
            self.request_stats['blocked_requests'] += 1
        else:
            self.request_stats['failed_requests'] += 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = (self.request_stats['avg_response_time'] * 
                     (self.request_stats['total_requests'] - 1) + response_time)
        self.request_stats['avg_response_time'] = total_time / self.request_stats['total_requests']
        
        # è®°å½•æ€§èƒ½å†å²
        self.performance_history.append({
            'timestamp': time.time(),
            'success': success,
            'response_time': response_time,
            'is_blocked': is_blocked
        })
        
        # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
        if len(self.performance_history) > 1000:
            self.performance_history = self.performance_history[-1000:]
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        stats = self.request_stats
        success_rate = (stats['successful_requests'] / stats['total_requests'] * 100 
                       if stats['total_requests'] > 0 else 0)
        block_rate = (stats['blocked_requests'] / stats['total_requests'] * 100 
                     if stats['total_requests'] > 0 else 0)
        
        return {
            'è¿è¡Œæ—¶é—´': f"{(time.time() - self.start_time)/60:.1f}åˆ†é’Ÿ",
            'æ€»è¯·æ±‚æ•°': stats['total_requests'],
            'æˆåŠŸç‡': f"{success_rate:.1f}%",
            'å°ç¦ç‡': f"{block_rate:.1f}%",
            'å¹³å‡å“åº”æ—¶é—´': f"{stats['avg_response_time']:.2f}ç§’",
            'æˆåŠŸç‡è­¦æˆ’çº¿': '95%' if success_rate < 95 else 'æ­£å¸¸',
            'å°ç¦ç‡è­¦æˆ’çº¿': '5%' if block_rate > 5 else 'æ­£å¸¸'
        }
    
    def should_alert(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦"""
        if self.request_stats['total_requests'] < 10:
            return False
            
        success_rate = (self.request_stats['successful_requests'] / 
                       self.request_stats['total_requests'])
        block_rate = (self.request_stats['blocked_requests'] / 
                     self.request_stats['total_requests'])
        
        return success_rate < 0.8 or block_rate > 0.1
```

## å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼ˆç´§æ€¥ä¼˜åŒ– - 1-2å¤©ï¼‰
1. **å®ç°é›¶é‡å¤æ•°æ®è·å–** - é¿å…ä¸€è‡´è¡ŒåŠ¨äººå’Œæœºæ„æŒä»“é‡å¤æŸ¥è¯¢
2. **æ·»åŠ åŸºç¡€è¯·æ±‚é¢‘ç‡æ§åˆ¶** - æ¯åˆ†é’Ÿä¸è¶…è¿‡20æ¬¡è¯·æ±‚
3. **å®ç°æ–‡ä»¶çº§ç¼“å­˜** - ç¼“å­˜è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯

### ç¬¬äºŒé˜¶æ®µï¼ˆæ€§èƒ½ä¼˜åŒ– - 3-5å¤©ï¼‰
1. **å®ç°è¯·æ±‚å¤´è½®æ¢ç³»ç»Ÿ** - å¤šå¥—User-Agentå’ŒReferer
2. **æ·»åŠ è‡ªé€‚åº”å»¶è¿Ÿæœºåˆ¶** - æ ¹æ®å“åº”æƒ…å†µåŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
3. **å®Œå–„é”™è¯¯æ¢å¤æœºåˆ¶** - æ™ºèƒ½é‡è¯•å’ŒIPè½®æ¢

### ç¬¬ä¸‰é˜¶æ®µï¼ˆé«˜çº§ä¼˜åŒ– - 1-2å‘¨ï¼‰
1. **å®ç°å¼‚æ­¥å¹¶å‘å¤„ç†** - å¤šçº¿ç¨‹/åç¨‹å¹¶è¡Œè·å–
2. **æ·»åŠ å¤šå±‚ç¼“å­˜ç³»ç»Ÿ** - L1/L2/L3ä¸‰çº§ç¼“å­˜
3. **éƒ¨ç½²å®æ—¶ç›‘æ§ç³»ç»Ÿ** - æ€§èƒ½ç›‘æ§å’Œè‡ªåŠ¨å‘Šè­¦

## é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡
- **APIè°ƒç”¨æ¬¡æ•°å‡å°‘**: ä»11æ¬¡/è‚¡ç¥¨é™è‡³3-4æ¬¡/è‚¡ç¥¨ï¼ˆå‡å°‘65%ï¼‰
- **æ‰§è¡Œæ—¶é—´ç¼©çŸ­**: æ•´ä½“å¤„ç†æ—¶é—´å‡å°‘70%ä»¥ä¸Š
- **å†…å­˜ä½¿ç”¨ä¼˜åŒ–**: é€šè¿‡ç¼“å­˜å‡å°‘é‡å¤æ•°æ®å­˜å‚¨

### ç¨³å®šæ€§æå‡
- **IPå°ç¦é£é™©é™ä½**: 90%ä»¥ä¸Šçš„å°ç¦äº‹ä»¶è‡ªåŠ¨æ¢å¤
- **æˆåŠŸç‡æå‡**: ä»80%æå‡è‡³95%ä»¥ä¸Š
- **é”™è¯¯æ¢å¤æ—¶é—´**: ä»åˆ†é’Ÿçº§é™è‡³ç§’çº§

### ç›‘æ§èƒ½åŠ›
- **å®æ—¶æ€§èƒ½ç›‘æ§**: æˆåŠŸç‡ã€å“åº”æ—¶é—´ã€å°ç¦ç‡å®æ—¶è·Ÿè¸ª
- **æ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ**: å¼‚å¸¸æƒ…å†µè‡ªåŠ¨æé†’
- **æ€§èƒ½å†å²åˆ†æ**: é•¿æœŸæ€§èƒ½è¶‹åŠ¿åˆ†æ

è¿™ä¸ªç²¾ç»†åŒ–æ–¹æ¡ˆé€šè¿‡é›¶é‡å¤è·å–ã€æ™ºèƒ½åçˆ¬ã€å¤šå±‚ç¼“å­˜å’Œå¼‚æ­¥å¤„ç†ï¼Œèƒ½å¤Ÿä»æ ¹æœ¬ä¸Šè§£å†³IPå°ç¦é—®é¢˜ï¼ŒåŒæ—¶å¤§å¹…æå‡æ‰§è¡Œæ•ˆç‡ã€‚
